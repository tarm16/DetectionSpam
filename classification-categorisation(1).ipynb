{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefad15d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:29:33.347472Z",
     "iopub.status.busy": "2023-09-22T02:29:33.346839Z",
     "iopub.status.idle": "2023-09-22T02:29:43.852853Z",
     "shell.execute_reply": "2023-09-22T02:29:43.851512Z"
    },
    "papermill": {
     "duration": 10.524173,
     "end_time": "2023-09-22T02:29:43.855932",
     "exception": false,
     "start_time": "2023-09-22T02:29:33.331759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import email\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2f830e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:29:43.883812Z",
     "iopub.status.busy": "2023-09-22T02:29:43.883063Z",
     "iopub.status.idle": "2023-09-22T02:29:44.840295Z",
     "shell.execute_reply": "2023-09-22T02:29:44.839166Z"
    },
    "papermill": {
     "duration": 0.973953,
     "end_time": "2023-09-22T02:29:44.843436",
     "exception": false,
     "start_time": "2023-09-22T02:29:43.869483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv(\"/kaggle/input/emailsclean/New_emailsdata.csv\")\n",
    "df1 = pd.read_csv(\"/kaggle/input/enroncleanobff/EnronDatasetProjectNew.csv\")\n",
    "df2 = pd.read_csv(\"/kaggle/input/bbc-text-clean/BBC-Text-Clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd073a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:29:44.870999Z",
     "iopub.status.busy": "2023-09-22T02:29:44.870613Z",
     "iopub.status.idle": "2023-09-22T02:29:44.905684Z",
     "shell.execute_reply": "2023-09-22T02:29:44.904315Z"
    },
    "papermill": {
     "duration": 0.052021,
     "end_time": "2023-09-22T02:29:44.908465",
     "exception": false,
     "start_time": "2023-09-22T02:29:44.856444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>emailclean</th>\n",
       "      <th>taux_obfuscation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>fyi \\n                      forwarded by lauri...</td>\n",
       "      <td>fyi forwarded by lauri allen hou ect on pm kim...</td>\n",
       "      <td>0.000673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>jackie \\nsince the inlet to  river plant is sh...</td>\n",
       "      <td>jackie since the inlet to river plant is shut ...</td>\n",
       "      <td>0.001221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "      <td>daren  sorry i forgot to include you\\n        ...</td>\n",
       "      <td>daren sorry forgot to include you forwarded by...</td>\n",
       "      <td>0.001010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>politics</td>\n",
       "      <td>pursuant to our recent confirmation i have dra...</td>\n",
       "      <td>pursuant to our recent confirmation have draft...</td>\n",
       "      <td>0.001546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>sport</td>\n",
       "      <td>details for the deal to be entered on jan     ...</td>\n",
       "      <td>details for the deal to be entered on jan buy ...</td>\n",
       "      <td>0.002096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label  category  \\\n",
       "0           0      0  business   \n",
       "1           1      0  business   \n",
       "2           2      0  politics   \n",
       "3           3      0  politics   \n",
       "4           4      0     sport   \n",
       "\n",
       "                                                text  \\\n",
       "0  fyi \\n                      forwarded by lauri...   \n",
       "1  jackie \\nsince the inlet to  river plant is sh...   \n",
       "2  daren  sorry i forgot to include you\\n        ...   \n",
       "3  pursuant to our recent confirmation i have dra...   \n",
       "4  details for the deal to be entered on jan     ...   \n",
       "\n",
       "                                          emailclean  taux_obfuscation  \n",
       "0  fyi forwarded by lauri allen hou ect on pm kim...          0.000673  \n",
       "1  jackie since the inlet to river plant is shut ...          0.001221  \n",
       "2  daren sorry forgot to include you forwarded by...          0.001010  \n",
       "3  pursuant to our recent confirmation have draft...          0.001546  \n",
       "4  details for the deal to be entered on jan buy ...          0.002096  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90332171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:29:44.937869Z",
     "iopub.status.busy": "2023-09-22T02:29:44.937439Z",
     "iopub.status.idle": "2023-09-22T02:29:44.950468Z",
     "shell.execute_reply": "2023-09-22T02:29:44.948847Z"
    },
    "papermill": {
     "duration": 0.031022,
     "end_time": "2023-09-22T02:29:44.953197",
     "exception": false,
     "start_time": "2023-09-22T02:29:44.922175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business' 'politics' 'sport' 'entertainment' 'tech']\n"
     ]
    }
   ],
   "source": [
    "unique_categories = df1[\"category\"].unique()\n",
    "print(unique_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c777df03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:29:44.981471Z",
     "iopub.status.busy": "2023-09-22T02:29:44.981054Z",
     "iopub.status.idle": "2023-09-22T02:29:44.995351Z",
     "shell.execute_reply": "2023-09-22T02:29:44.994128Z"
    },
    "papermill": {
     "duration": 0.031074,
     "end_time": "2023-09-22T02:29:44.997699",
     "exception": false,
     "start_time": "2023-09-22T02:29:44.966625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catégorie : entertainment  - Nombre d'instances : 1549\n",
      "Catégorie : politics  - Nombre d'instances : 1256\n",
      "Catégorie : business  - Nombre d'instances : 1189\n",
      "Catégorie : sport  - Nombre d'instances : 1115\n",
      "Catégorie : tech  - Nombre d'instances : 1001\n"
     ]
    }
   ],
   "source": [
    "# Importer la bibliothèque Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Compter le nombre d'instances pour chaque catégorie\n",
    "counts = df1[\"category\"].value_counts()\n",
    "\n",
    "# Afficher le nombre d'instances pour chaque catégorie\n",
    "for category, count in counts.items():\n",
    "    print(\"Catégorie :\", category, \" - Nombre d'instances :\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "707adc26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:29:45.029376Z",
     "iopub.status.busy": "2023-09-22T02:29:45.028946Z",
     "iopub.status.idle": "2023-09-22T02:29:45.050497Z",
     "shell.execute_reply": "2023-09-22T02:29:45.049301Z"
    },
    "papermill": {
     "duration": 0.040869,
     "end_time": "2023-09-22T02:29:45.053829",
     "exception": false,
     "start_time": "2023-09-22T02:29:45.012960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supprimer les instances avec emailclean vide ou NaN\n",
    "df1 = df1.dropna(subset=['emailclean'])\n",
    "df1 = df1[df1['emailclean'] != '']\n",
    "\n",
    "# Réinitialiser les index\n",
    "df1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276f6fc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:29:45.083325Z",
     "iopub.status.busy": "2023-09-22T02:29:45.082893Z",
     "iopub.status.idle": "2023-09-22T02:29:45.090266Z",
     "shell.execute_reply": "2023-09-22T02:29:45.089008Z"
    },
    "papermill": {
     "duration": 0.025683,
     "end_time": "2023-09-22T02:29:45.093213",
     "exception": false,
     "start_time": "2023-09-22T02:29:45.067530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6110, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b4a86",
   "metadata": {
    "papermill": {
     "duration": 0.013343,
     "end_time": "2023-09-22T02:29:45.120357",
     "exception": false,
     "start_time": "2023-09-22T02:29:45.107014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "partie détéction et calcule du taux d'obfuscation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724ee285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:29:45.150846Z",
     "iopub.status.busy": "2023-09-22T02:29:45.149359Z",
     "iopub.status.idle": "2023-09-22T02:29:45.722361Z",
     "shell.execute_reply": "2023-09-22T02:29:45.720842Z"
    },
    "papermill": {
     "duration": 0.590424,
     "end_time": "2023-09-22T02:29:45.724437",
     "exception": true,
     "start_time": "2023-09-22T02:29:45.134013",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         body \u001b[38;5;241m=\u001b[39m email_message\u001b[38;5;241m.\u001b[39mget_payload()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m body\n\u001b[0;32m---> 12\u001b[0m emails \u001b[38;5;241m=\u001b[39m \u001b[43mdf3\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexteTest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     13\u001b[0m bodies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m email_str \u001b[38;5;129;01min\u001b[39;00m emails:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df3' is not defined"
     ]
    }
   ],
   "source": [
    "import email\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "def extract_body(email_message):\n",
    "    if email_message.is_multipart():\n",
    "        # If the email has multiple parts, extract the text from the first part\n",
    "        body = email_message.get_payload(0).get_payload()\n",
    "    else:\n",
    "        body = email_message.get_payload()\n",
    "    return body\n",
    "\n",
    "emails = df3['texteTest'].tolist()\n",
    "bodies = []\n",
    "\n",
    "for email_str in emails:\n",
    "    email_message = email.message_from_string(email_str)\n",
    "    body = extract_body(email_message)\n",
    "    bodies.append(body)\n",
    "\n",
    "def detect_obfuscation(body):\n",
    "    # Obfuscation detection patterns\n",
    "    patterns = [\n",
    "    r\"\\b(?:make money|earn money|have money|get rich)\\b\",  # Phrases liées à l'argent\n",
    "    r\"\\b(?:lose weight|diet|fitness)\\b\",  # Phrases liées à la perte de poids ou au régime\n",
    "    r\"\\b(?:free|discount|offer)\\b\",  # Phrases liées aux offres gratuites ou aux réductions\n",
    "    r\"\\b(?:limited time|exclusive deal)\\b\",  # Phrases liées aux offres limitées dans le temps ou exclusives\n",
    "    r\"\\b(?:lottery|prize|win)\\b\",  # Phrases liées aux loteries, prix ou gains\n",
    "    r\"\\b(?:urgent|important|attention)\\b\",  # Phrases avec des termes d'urgence ou d'importance\n",
    "    r\"\\b(?:viagra|cialis|medication)\\b\",  # Phrases liées à des médicaments ou produits pharmaceutiques\n",
    "    r\"\\b(?:credit card|bank account)\\b\",  # Phrases liées aux cartes de crédit ou aux comptes bancaires\n",
    "    r\"\\b(?:refund|money back)\\b\",  # Phrases liées aux remboursements ou aux retours d'argent\n",
    "    r\"\\b(?:exclusive offer|secret formula)\\b\",  # Phrases liées à des offres exclusives ou à des formules secrètes\n",
    "    r\"\\b[\\w\\.-]+@[\\w\\.-]+\\b\",  # Simple email address\n",
    "    r\"\\b(?:https?://)?(?:[A-Za-z0-9-]+\\.)*[A-Za-z0-9-]+\\.[A-Za-z]{2,}\\S*\\b\",  # URL\n",
    "    r\"\\\\[uU][a-fA-F0-9]{4}\",  # Unicode characters\n",
    "    \"[^\\w\\s]\",  # Modèles de caractères spéciaux\n",
    "    r\"\\d\",  # Modèles de nombres\n",
    "    r\"\\s{2,}\",  # Modèles d'espaces inattendus\n",
    "    r\"pharmacie en ligne|gratuit|promotion|gagner de l'argent rapidement\",  # Modèles de mots-clés\n",
    "    r\"bit\\.ly|goo\\.gl\",  # Modèles de liens raccourcis\n",
    "    r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",  # Modèles d'adresses IP\n",
    "    r\"\\b[a-zA-Z]+[^.?!]*[.?!]\",  # Modèles d'erreurs grammaticales\n",
    "    r\"[^.?!]*[.?!]\",  # Modèles de phrases incomplètes\n",
    "    r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\",  # Modèles d'adresses e-mail\n",
    "    r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\",  # Modèles de numéros de téléphone\n",
    "    r\"\\b\\d+\\s+[A-Za-z]+\\s+[A-Za-z]+\\b\",  # Modèles d'adresses postales\n",
    "    ]\n",
    " \n",
    "\n",
    "    # Search for obfuscation patterns\n",
    "    obfuscated = []\n",
    "    for pattern in patterns:\n",
    "        obfuscated += re.findall(pattern, body)\n",
    "\n",
    "    # Return True if obfuscation patterns were found, False otherwise\n",
    "    return bool(obfuscated)\n",
    "\n",
    "obfuscated_emails = []\n",
    "\n",
    "for body in bodies:\n",
    "    if detect_obfuscation(body):\n",
    "        obfuscated_emails.append(body)\n",
    "\n",
    "print(len(obfuscated_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1c6f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:02:26.114796Z",
     "iopub.status.busy": "2023-09-22T02:02:26.113951Z",
     "iopub.status.idle": "2023-09-22T02:02:26.477676Z",
     "shell.execute_reply": "2023-09-22T02:02:26.476970Z",
     "shell.execute_reply.started": "2023-09-22T02:02:26.114757Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculer le taux d'obfuscation pour chaque email\n",
    "\n",
    "taux_obfuscation = []\n",
    "\n",
    "for email_str in df3['texteTest']:\n",
    "\n",
    "    email_message = email.message_from_string(email_str)\n",
    "\n",
    "    body = extract_body(email_message)\n",
    "\n",
    "    nb_obfuscations = detect_obfuscation(body)\n",
    "\n",
    "    taux_obfuscation.append(nb_obfuscations / len(body) if len(body) > 0 else 0)  # éviter la division par zéro\n",
    "\n",
    "\n",
    "\n",
    "# ajouter la colonne \"taux_obfuscation\" au DataFrame\n",
    "\n",
    "df3['taux_obfuscation'] = taux_obfuscation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b040a9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spam_count = df1[df1[\"label\"] == 1].shape[0] \n",
    "ham_count = df1[df1[\"label\"] == 0].shape[0]\n",
    "\n",
    "print(\"Nombre d'instances spam :\", spam_count) \n",
    "print(\"Nombre d'instances ham :\", ham_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa682efa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "PARTIE CLASSIFICATION DE SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a405ce3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:35:25.431952Z",
     "iopub.status.busy": "2023-09-22T01:35:25.431490Z",
     "iopub.status.idle": "2023-09-22T01:35:41.384571Z",
     "shell.execute_reply": "2023-09-22T01:35:41.383668Z",
     "shell.execute_reply.started": "2023-09-22T01:35:25.431916Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "# Chargement du modèle Universal Sentence Encoder\n",
    "model_path = \"/kaggle/input/universal-sentence-encoder/tensorflow2/universal-sentence-encoder/2\"\n",
    "embed = hub.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f38fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:35:44.734605Z",
     "iopub.status.busy": "2023-09-22T01:35:44.734198Z",
     "iopub.status.idle": "2023-09-22T01:35:44.768833Z",
     "shell.execute_reply": "2023-09-22T01:35:44.767988Z",
     "shell.execute_reply.started": "2023-09-22T01:35:44.734572Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "# Transforming catagories label to \n",
    "labels = df1['category'].astype(str)\n",
    "\n",
    "lb = LabelBinarizer().fit(list(set(labels.tolist())))\n",
    "train_labels_categoryCat = lb.transform(df1['category'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1112410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:35:53.183558Z",
     "iopub.status.busy": "2023-09-22T01:35:53.183167Z",
     "iopub.status.idle": "2023-09-22T01:35:53.202919Z",
     "shell.execute_reply": "2023-09-22T01:35:53.202072Z",
     "shell.execute_reply.started": "2023-09-22T01:35:53.183529Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts_category = df1['emailclean']\n",
    "train_labels_category = df1['label']\n",
    "train_category_obf_rate = df1['taux_obfuscation']\n",
    "\n",
    "    # Diviser les données en ensembles d'entraînement, de validation et de test\n",
    "train_texts, test_texts, train_labels, test_labels, train_cat, test_cat, train_obf_rate, test_obf_rate = train_test_split(\n",
    "train_texts_category, train_labels_category, train_labels_categoryCat, train_category_obf_rate,\n",
    "test_size=0.3, stratify=train_labels_category, random_state=42)\n",
    "\n",
    "train_texts, val_texts, train_cat, val_cat, train_labels, val_labels, train_obf_rate, val_obf_rate = train_test_split(\n",
    "train_texts, train_cat, train_labels, train_obf_rate,\n",
    "test_size=0.3, stratify=train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3496c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:36:27.702885Z",
     "iopub.status.busy": "2023-09-22T01:36:27.702486Z",
     "iopub.status.idle": "2023-09-22T01:38:23.478985Z",
     "shell.execute_reply": "2023-09-22T01:38:23.478065Z",
     "shell.execute_reply.started": "2023-09-22T01:36:27.702856Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Préparer les données d'entraînement, de validation et de test avec l'embedding Universal Sentence Encoder\n",
    "train_features = embed.signatures['serving_default'](tf.convert_to_tensor(train_texts))['outputs']\n",
    "val_features = embed.signatures['serving_default'](tf.convert_to_tensor(val_texts))['outputs']\n",
    "test_features = embed.signatures['serving_default'](tf.convert_to_tensor(test_texts))['outputs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b8081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:39:07.850390Z",
     "iopub.status.busy": "2023-09-22T01:39:07.849335Z",
     "iopub.status.idle": "2023-09-22T01:41:54.988452Z",
     "shell.execute_reply": "2023-09-22T01:41:54.987372Z",
     "shell.execute_reply.started": "2023-09-22T01:39:07.850344Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Attention\n",
    "# Définir les couches d'entrée pour les textes et le taux d'obfuscation\n",
    "input_text = tf.keras.layers.Input(shape=(train_features.shape[1],), name='text')#sa permet au model de s'adapter a la dimensionalité d'origine\n",
    "input_obf_rate = tf.keras.layers.Input(shape=(1,), name='obf_rate')\n",
    "# Architecture de categorisation\n",
    "y = tf.keras.layers.Dense(256, activation='relu', name='cat-dense1')(input_text)\n",
    "y = tf.keras.layers.Dropout(0.3, name='cat-dropout1')(y)\n",
    "y = tf.keras.layers.Dense(128, activation='relu', name='cat-dense2')(y)\n",
    "y = tf.keras.layers.Dropout(0.3, name='cat-dropout2')(y)\n",
    "y = tf.keras.layers.Dense(64, activation='relu', name='cat-dense3')(y)\n",
    "y = tf.keras.layers.Dropout(0.3, name='cat-dropout3')(y)\n",
    "\n",
    "cat = tf.keras.layers.Dense(5, activation='softmax', name='cat')(y)\n",
    "CategorisationModel = tf.keras.models.Model(inputs=input_text, outputs=cat)\n",
    "CategorisationModel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics ='categorical_accuracy') \n",
    "# Train the Model\n",
    "\n",
    "history1 = CategorisationModel.fit(train_features, train_cat, epochs=120, batch_size=32)\n",
    "\n",
    "\n",
    "# Obtenir la sortie de l'avant-dernière couche du modèle de catégorisation\n",
    "output_categorisation = CategorisationModel.layers[-2].output\n",
    "output_categorisation = tf.keras.layers.Dense(64, activation='relu')(output_categorisation)\n",
    "\n",
    "\n",
    "    # Concaténer les couches d'entrée\n",
    "concatenated_inputs = tf.keras.layers.concatenate([input_text, input_obf_rate])\n",
    "\n",
    "    # Définir le reste du modèle\n",
    "x = tf.keras.layers.Dense(256, activation='relu', name='spam-dense1')(concatenated_inputs)\n",
    "x = tf.keras.layers.Dropout(0.3, name='spam-dropout1')(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu', name='spam-dense2')(x)\n",
    "x = tf.keras.layers.Dropout(0.3, name='spam-dropout2')(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu', name='spam-dense3')(x)\n",
    "x = tf.keras.layers.Dropout(0.3, name='spam-dropout3')(x)\n",
    "# Appliquer une couche d'attention\n",
    "attention = Attention()([x, output_categorisation])\n",
    "\n",
    "\n",
    "# Ajouter une couche dense pour fusionner les sorties\n",
    "merged_output = tf.keras.layers.Dense(64, activation='relu', name='merged_dense')(attention)\n",
    "\n",
    "# Couche de sortie\n",
    "outputSpam = tf.keras.layers.Dense(1, activation='sigmoid', name='output-spam')(merged_output)\n",
    "\n",
    "# Créer le modèle final\n",
    "spam_model = tf.keras.models.Model(inputs=[input_text, input_obf_rate], outputs=outputSpam)\n",
    "\n",
    "# Compiler et entraîner le modèle\n",
    "spam_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = spam_model.fit([train_features, train_obf_rate], train_labels, epochs=100, batch_size=32,\n",
    "                         validation_data=([val_features, val_obf_rate], val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28240a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:42:36.533785Z",
     "iopub.status.busy": "2023-09-22T01:42:36.533336Z",
     "iopub.status.idle": "2023-09-22T01:42:36.961412Z",
     "shell.execute_reply": "2023-09-22T01:42:36.960347Z",
     "shell.execute_reply.started": "2023-09-22T01:42:36.533749Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "# Évaluer le modèle sur les données de test\n",
    "test_loss, test_accuracy = spam_model.evaluate([test_features, test_obf_rate], test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Prédire les étiquettes pour les données de test\n",
    "SuperModelpred = spam_model.predict([test_features, test_obf_rate])\n",
    "SuperModelpred = np.round(SuperModelpred).flatten()\n",
    "\n",
    "    # Calculer et afficher les métriques\n",
    "print(f\"roc_auc_score : {roc_auc_score(test_labels, SuperModelpred)}\")\n",
    "print(f\"f1_score  : {f1_score(test_labels, SuperModelpred)}\")\n",
    "print(f\"recall_score : {recall_score(test_labels, SuperModelpred)}\")\n",
    "print(f\"precision_score : {precision_score(test_labels, SuperModelpred)}\")\n",
    "\n",
    "    # Afficher la matrice de confusion\n",
    "cm = confusion_matrix(test_labels, SuperModelpred)\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6949a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T01:44:37.236881Z",
     "iopub.status.busy": "2023-09-22T01:44:37.236371Z",
     "iopub.status.idle": "2023-09-22T01:44:37.255980Z",
     "shell.execute_reply": "2023-09-22T01:44:37.255111Z",
     "shell.execute_reply.started": "2023-09-22T01:44:37.236844Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"/kaggle/input/textspam/text_spam (1).csv\")\n",
    "df3['texteTest'] = df3.iloc[:, 0]\n",
    "df3 = df3.drop(df3.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd33331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:04:23.791733Z",
     "iopub.status.busy": "2023-09-22T02:04:23.790643Z",
     "iopub.status.idle": "2023-09-22T02:04:24.011609Z",
     "shell.execute_reply": "2023-09-22T02:04:24.010719Z",
     "shell.execute_reply.started": "2023-09-22T02:04:23.791682Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = embed.signatures['serving_default'](tf.convert_to_tensor(df3['texteTest']))['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db7c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:07:31.651499Z",
     "iopub.status.busy": "2023-09-22T02:07:31.651014Z",
     "iopub.status.idle": "2023-09-22T02:07:31.778729Z",
     "shell.execute_reply": "2023-09-22T02:07:31.777560Z",
     "shell.execute_reply.started": "2023-09-22T02:07:31.651461Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3_labels = spam_model.predict([features, df3[\"taux_obfuscation\"]])\n",
    "df3_labels = np.round(df3_labels).flatten()\n",
    "\n",
    "# Ajouter les étiquettes prédites à df3\n",
    "df3[\"spam\"] = df3_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0245a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:07:46.524998Z",
     "iopub.status.busy": "2023-09-22T02:07:46.524552Z",
     "iopub.status.idle": "2023-09-22T02:07:46.540021Z",
     "shell.execute_reply": "2023-09-22T02:07:46.538517Z",
     "shell.execute_reply.started": "2023-09-22T02:07:46.524955Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f146c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T02:09:53.677515Z",
     "iopub.status.busy": "2023-09-22T02:09:53.676317Z",
     "iopub.status.idle": "2023-09-22T02:09:53.686015Z",
     "shell.execute_reply": "2023-09-22T02:09:53.684817Z",
     "shell.execute_reply.started": "2023-09-22T02:09:53.677474Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spam_count = df3[df3[\"spam\"] == 1].shape[0] \n",
    "ham_count = df3[df3[\"spam\"] == 0].shape[0]\n",
    "\n",
    "print(\"Nombre d'instances spam :\", spam_count) \n",
    "print(\"Nombre d'instances ham :\", ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2cdc1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d327ca6c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "CODE SUIVANT avec boucle IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fe0ae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "spam_models = {}\n",
    "\n",
    "for categorie in ['business', 'politics', 'tech', 'sport', 'entertainment']:\n",
    "    # Select the data for this category\n",
    "    df_categorie = df1[df1['category'] == categorie]\n",
    "\n",
    "    train_texts_categorie = df_categorie['emailclean']\n",
    "    train_labels_categorie = df_categorie['label']\n",
    "    train_cat_categorie = df_categorie['category']\n",
    "    train_category_obf_rate = df_categorie['taux_obfuscation']\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_texts, test_texts, train_labels, test_labels, train_cat, test_cat, train_obf_rate, test_obf_rate = train_test_split(\n",
    "        train_texts_categorie, train_labels_categorie, train_cat_categorie, train_category_obf_rate, \n",
    "        test_size=0.3, stratify=train_labels_categorie, random_state=42\n",
    "    )\n",
    "\n",
    "    train_texts, val_texts, train_labels, val_labels, train_obf_rate, val_obf_rate = train_test_split(\n",
    "        train_texts, train_labels, train_obf_rate,\n",
    "        test_size=0.3, stratify=train_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create an instance of TfidfVectorizer and fit it on the training data\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_features = vectorizer.fit_transform(train_texts)\n",
    "    test_features = vectorizer.transform(test_texts)\n",
    "\n",
    "    # Define inputs for the model\n",
    "    text_input = tf.keras.layers.Input(shape=(train_features.shape[1],), dtype=tf.float32, name='text')\n",
    "    obf_rate_input = tf.keras.layers.Input(shape=(1,), dtype=tf.float32, name='obf_rate')\n",
    "\n",
    "    # Concatenate the features\n",
    "    concatenated = tf.keras.layers.Concatenate()([text_input, obf_rate_input])\n",
    "\n",
    "    dense = Dense(256, activation='relu', name='dense')(concatenated)\n",
    "    dropout = Dropout(0.1, name='dropout')(dense)\n",
    "    output = Dense(1, activation='sigmoid', name='output')(dropout)\n",
    "\n",
    "    spam_model = tf.keras.Model(inputs=[text_input, obf_rate_input], outputs=[output])\n",
    "\n",
    "    # Compile and train the model\n",
    "    spam_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Convert the features to tensors\n",
    "    train_features_tensor = tf.convert_to_tensor(train_features.toarray(), dtype=tf.float32)\n",
    "    val_features_tensor = tf.convert_to_tensor(vectorizer.transform(val_texts).toarray(), dtype=tf.float32)\n",
    "    test_features_tensor = tf.convert_to_tensor(test_features.toarray(), dtype=tf.float32)\n",
    "\n",
    "    # Use the cache method to cache the data in memory to speed up the model training\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(((train_features_tensor, train_obf_rate), train_labels)).cache().batch(32)\n",
    "    val_data = tf.data.Dataset.from_tensor_slices(((val_features_tensor, val_obf_rate), val_labels)).cache().batch(32)\n",
    "    test_data = tf.data.Dataset.from_tensor_slices(((test_features_tensor, test_obf_rate), test_labels)).batch(32)\n",
    "\n",
    "    # Define the EarlyStopping callback\n",
    "    history = spam_model.fit(train_data, epochs=40, batch_size=32, validation_data=val_data)\n",
    "    \n",
    "    \n",
    "    test_loss, test_accuracy = spam_model.evaluate(test_data)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    y_pred = spam_model.predict(test_data)\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "    \n",
    "    \n",
    "   \n",
    "    # Calculate and print the metrics\n",
    "    print(f\"roc_auc_score for {categorie}: {roc_auc_score(test_labels, y_pred)}\")\n",
    "    print(f\"f1_score for {categorie}: {f1_score(test_labels, y_pred)}\")\n",
    "    print(f\"recall_score for {categorie}: {recall_score(test_labels, y_pred)}\")\n",
    "    print(f\"precision_score for {categorie}: {precision_score(test_labels, y_pred)}\")\n",
    "    # Print the confusion matrix\n",
    "    cm = confusion_matrix(test_labels, y_pred)\n",
    "    print(f\"Confusion Matrix for {categorie}:\")\n",
    "    print(cm)\n",
    "    \n",
    "\n",
    "    spam_models[categorie] = spam_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1c560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T18:41:53.326712Z",
     "iopub.status.busy": "2023-07-25T18:41:53.326224Z",
     "iopub.status.idle": "2023-07-25T18:44:11.378268Z",
     "shell.execute_reply": "2023-07-25T18:44:11.376813Z",
     "shell.execute_reply.started": "2023-07-25T18:41:53.326676Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, MultiHeadAttention \n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "spam_models = {}\n",
    "categories = ['business', 'politics', 'tech', 'sport', 'entertainment']\n",
    "\n",
    "for category in categories:\n",
    "    # Sélectionner les données pour cette catégorie\n",
    "    df_category = df1[df1['category'] == category]\n",
    "\n",
    "    train_texts_category = df_category['emailclean']\n",
    "    train_labels_category = df_category['label']\n",
    "    train_cat_category = df_category['category']\n",
    "    train_category_obf_rate = df_category['taux_obfuscation']\n",
    "\n",
    "    # Diviser les données en ensembles d'entraînement, de validation et de test\n",
    "    train_texts, test_texts, train_labels, test_labels, train_cat, test_cat, train_obf_rate, test_obf_rate = train_test_split(\n",
    "        train_texts_category, train_labels_category, train_cat_category, train_category_obf_rate,\n",
    "        test_size=0.3, stratify=train_labels_category, random_state=42\n",
    "    )\n",
    "\n",
    "    train_texts, val_texts, train_labels, val_labels, train_obf_rate, val_obf_rate = train_test_split(\n",
    "        train_texts, train_labels, train_obf_rate,\n",
    "        test_size=0.3, stratify=train_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    # Préparer les données d'entraînement, de validation et de test avec l'embedding Universal Sentence Encoder\n",
    "    train_features = embed.signatures['serving_default'](tf.convert_to_tensor(train_texts))['outputs']\n",
    "    val_features = embed.signatures['serving_default'](tf.convert_to_tensor(val_texts))['outputs']\n",
    "    test_features = embed.signatures['serving_default'](tf.convert_to_tensor(test_texts))['outputs']\n",
    "\n",
    "    # Définir les couches d'entrée pour les textes et le taux d'obfuscation\n",
    "    input_text = tf.keras.layers.Input(shape=(train_features.shape[1],), name='text')\n",
    "    input_obf_rate = tf.keras.layers.Input(shape=(1,), name='obf_rate')\n",
    "\n",
    "    # Concaténer les couches d'entrée\n",
    "    concatenated_inputs = tf.keras.layers.concatenate([input_text, input_obf_rate])\n",
    "\n",
    "\n",
    "    # Définir le reste du modèle\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', name='dense1')(concatenated_inputs)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='dropout1')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='dense2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='dropout2')(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense3')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3, name='dropout3')(x)\n",
    "\n",
    "    # Ajouter une couche d'expansion pour ajuster la dimension du tenseur\n",
    "    x = tf.keras.layers.Reshape((1, -1))(x)\n",
    "    # Ajouter une couche Multi-Head Attention\n",
    "    attention = MultiHeadAttention(num_heads=8, key_dim=120)\n",
    "    x = attention(x, x)\n",
    "\n",
    "    # Réduire les dimensions du tenseur pour la couche BatchNormalization\n",
    "    x = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    spam_model = tf.keras.models.Model(inputs=[input_text, input_obf_rate], outputs=output)\n",
    "    # Compiler et entraîner le modèle\n",
    "    spam_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = spam_model.fit(\n",
    "        [train_features, train_obf_rate], train_labels,\n",
    "        epochs=30, batch_size=32,\n",
    "        validation_data=([val_features, val_obf_rate], val_labels)\n",
    "    )\n",
    "\n",
    "    # Évaluer le modèle sur les données de test\n",
    "    test_loss, test_accuracy = spam_model.evaluate([test_features, test_obf_rate], test_labels)\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Prédire les étiquettes pour les données de test\n",
    "    y_pred = spam_model.predict([test_features, test_obf_rate])\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "\n",
    "    # Calculer et afficher les métriques\n",
    "    print(f\"roc_auc_score for {category}: {roc_auc_score(test_labels, y_pred)}\")\n",
    "    print(f\"f1_score for {category}: {f1_score(test_labels, y_pred)}\")\n",
    "    print(f\"recall_score for {category}: {recall_score(test_labels, y_pred)}\")\n",
    "    print(f\"precision_score for {category}: {precision_score(test_labels, y_pred)}\")\n",
    "\n",
    "    # Afficher la matrice de confusion\n",
    "    cm = confusion_matrix(test_labels, y_pred)\n",
    "    print(f\"Confusion Matrix for {category}:\")\n",
    "    print(cm)\n",
    "\n",
    "    spam_models[category] = spam_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94357866",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T18:44:20.400614Z",
     "iopub.status.busy": "2023-07-25T18:44:20.400163Z",
     "iopub.status.idle": "2023-07-25T18:44:20.665584Z",
     "shell.execute_reply": "2023-07-25T18:44:20.663826Z",
     "shell.execute_reply.started": "2023-07-25T18:44:20.400580Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for category, model in spam_models.items():\n",
    "    print(f\"Architecture du modèle pour la catégorie '{category}':\")\n",
    "    print(model.summary())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df844e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-25T18:59:30.575603Z",
     "iopub.status.busy": "2023-07-25T18:59:30.575222Z",
     "iopub.status.idle": "2023-07-25T18:59:30.713441Z",
     "shell.execute_reply": "2023-07-25T18:59:30.712189Z",
     "shell.execute_reply.started": "2023-07-25T18:59:30.575569Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Supposons que votre modèle s'appelle \"spam_model\"\n",
    "plot_model(spam_model, to_file='architecture.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a87a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:39:43.460698Z",
     "iopub.status.busy": "2023-06-08T15:39:43.459383Z",
     "iopub.status.idle": "2023-06-08T15:42:18.133806Z",
     "shell.execute_reply": "2023-06-08T15:42:18.132652Z",
     "shell.execute_reply.started": "2023-06-08T15:39:43.460646Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "spam_models = {}\n",
    "\n",
    "for categorie in ['business', 'politics', 'tech', 'sport', 'entertainment']:\n",
    "    # Select the data for this category\n",
    "    df_categorie = df1[df1['category'] == categorie]\n",
    "\n",
    "    train_texts_categorie = df_categorie['text']\n",
    "    train_labels_categorie = df_categorie['label']\n",
    "    train_cat_categorie = df_categorie['category']\n",
    "    train_category_obf_rate = df_categorie['taux_obfuscation']\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_texts, test_texts, train_labels, test_labels, train_cat, test_cat, train_obf_rate, test_obf_rate = train_test_split(\n",
    "        train_texts_categorie, train_labels_categorie, train_cat_categorie, train_category_obf_rate, \n",
    "        test_size=0.3, stratify=train_labels_categorie, random_state=42\n",
    "    )\n",
    "\n",
    "    train_texts, val_texts, train_labels, val_labels, train_obf_rate, val_obf_rate = train_test_split(\n",
    "        train_texts, train_labels, train_obf_rate,\n",
    "        test_size=0.3, stratify=train_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create an instance of CountVectorizer and fit it on the training data\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count_vectorizer.fit(train_texts)\n",
    "\n",
    "    # Conversion of texts to vector representations using CountVectorizer\n",
    "    train_features_count = count_vectorizer.transform(train_texts).toarray()\n",
    "    val_features_count = count_vectorizer.transform(val_texts).toarray()\n",
    "    test_features_count = count_vectorizer.transform(test_texts).toarray()\n",
    "\n",
    "    # Concatenation of CountVectorizer vector representations with other features\n",
    "    train_features_concat = np.concatenate([train_features_count, train_obf_rate[:, np.newaxis]], axis=1)\n",
    "    val_features_concat = np.concatenate([val_features_count, val_obf_rate[:, np.newaxis]], axis=1)\n",
    "    test_features_concat = np.concatenate([test_features_count, test_obf_rate[:, np.newaxis]], axis=1)\n",
    "\n",
    "    # Define the model architecture\n",
    "    spam_model = tf.keras.Sequential()\n",
    "    spam_model.add(tf.keras.layers.Dense(256, activation='relu', input_shape=(train_features_concat.shape[1],)))\n",
    "    spam_model.add(tf.keras.layers.Dropout(0.1))\n",
    "    spam_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    spam_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = spam_model.fit(train_features_concat, train_labels, epochs=40, batch_size=32, validation_data=(val_features_concat, val_labels))\n",
    " \n",
    "    # Evaluation du modèle avec les données de test\n",
    "    test_loss, test_accuracy = spam_model.evaluate(test_features_concat, test_labels)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    # Prédiction des classes avec le modèle\n",
    "    y_pred = spam_model.predict(test_features_concat)\n",
    "    y_pred = np.round(y_pred).flatten()\n",
    "    # Calculate and print the metrics\n",
    "    print(f\"roc_auc_score for {categorie}: {roc_auc_score(test_labels, y_pred)}\")\n",
    "    print(f\"f1_score for {categorie}: {f1_score(test_labels, y_pred)}\")\n",
    "    print(f\"recall_score for {categorie}: {recall_score(test_labels, y_pred)}\")\n",
    "    print(f\"precision_score for {categorie}: {precision_score(test_labels, y_pred)}\")\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    cm = confusion_matrix(test_labels, y_pred)\n",
    "    print(f\"Confusion Matrix for {categorie}:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Enregistrer le modèle pour cette catégorie\n",
    "    spam_models[categorie] = spam_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75568acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T04:09:50.359251Z",
     "iopub.status.busy": "2023-06-09T04:09:50.358165Z",
     "iopub.status.idle": "2023-06-09T04:09:50.452457Z",
     "shell.execute_reply": "2023-06-09T04:09:50.451520Z",
     "shell.execute_reply.started": "2023-06-09T04:09:50.359200Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pydot\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "# Générer une représentation graphique du modèle\n",
    "dot = model_to_dot(spam_model)\n",
    "png_bytes = dot.create_png(prog='dot')\n",
    "\n",
    "with open(f\"{category}_model.png\", \"wb\") as f:\n",
    "    f.write(png_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9312b27e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T04:11:11.304721Z",
     "iopub.status.busy": "2023-06-09T04:11:11.304310Z",
     "iopub.status.idle": "2023-06-09T04:11:11.526632Z",
     "shell.execute_reply": "2023-06-09T04:11:11.525591Z",
     "shell.execute_reply.started": "2023-06-09T04:11:11.304685Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# Afficher la représentation graphique du modèle\n",
    "plt.figure(figsize=(10, 10))\n",
    "img = plt.imread(f\"{category}_model.png\")\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8212f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-09T03:11:46.991166Z",
     "iopub.status.busy": "2023-06-09T03:11:46.988294Z",
     "iopub.status.idle": "2023-06-09T03:11:47.033268Z",
     "shell.execute_reply": "2023-06-09T03:11:47.031748Z",
     "shell.execute_reply.started": "2023-06-09T03:11:46.991122Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Créer un répertoire pour enregistrer les modèles\n",
    "models_directory = '/kaggle/working/'\n",
    "import tensorflow as tf\n",
    "os.makedirs(models_directory, exist_ok=True)\n",
    "\n",
    "# Entraîner et enregistrer les modèles\n",
    "for category, model in spam_models.items():\n",
    "    history = model.history\n",
    "    epochs = range(1, len(history['accuracy']) + 1)\n",
    "\n",
    "    # Enregistrer le modèle\n",
    "    model.save(f'{models_directory}/{category}.h5')\n",
    "\n",
    "    # Tracer les courbes de précision et de perte\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, history.history['accuracy'], 'r', label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'b', label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy - {category}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{models_directory}/{category}_accuracy.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, history['loss'], 'r', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'b', label='Validation Loss')\n",
    "    plt.title(f'Loss - {categorie}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{models_directory}/{category}_loss.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd741c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_counts = test_cat.value_counts()\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f058ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "spam = []\n",
    "\n",
    "for i in range(len(Xnew)):\n",
    "    if Znew[i] == 'business':\n",
    "        ar = np.array([Xnew[i]])\n",
    "        df10 = pd.DataFrame(ar, columns=['text'])\n",
    "        new_text_vector = vectorizer.transform(df10['text']).toarray()\n",
    "        y_predicted_bus = loaded_model.predict(new_text_vector)\n",
    "        y_predicted_bus = y_predicted_bus.flatten()\n",
    "        spam.append(np.where(y_predicted_bus > 0.5, 1, 0))\n",
    "    elif Znew[i] == 'tech':\n",
    "        ar = np.array([Xnew[i]])\n",
    "        df10 = pd.DataFrame(ar, columns=['text'])\n",
    "        new_text_vector = vectorizer.transform(df10['text']).toarray()\n",
    "        y_predicted_tech = loaded_model.predict(new_text_vector)\n",
    "        y_predicted_tech = y_predicted_tech.flatten()\n",
    "        spam.append(np.where(y_predicted_tech > 0.5, 1, 0))\n",
    "    elif Znew[i] == 'entertainment':\n",
    "        ar = np.array([Xnew[i]])\n",
    "        df10 = pd.DataFrame(ar, columns=['text'])\n",
    "        new_text_vector = vectorizer.transform(df10['text']).toarray()\n",
    "        y_predicted_Entert = loaded_model.predict(new_text_vector)\n",
    "        y_predicted_Entert = y_predicted_Entert.flatten()\n",
    "        spam.append(np.where(y_predicted_Entert > 0.5, 1, 0))\n",
    "    elif Znew[i] == 'sport':\n",
    "        ar = np.array([Xnew[i]])\n",
    "        df10 = pd.DataFrame(ar, columns=['text'])\n",
    "        new_text_vector = vectorizer.transform(df10['text']).toarray()\n",
    "        y_predicted_spo = loaded_model.predict(new_text_vector)\n",
    "        y_predicted_spo = y_predicted_spo.flatten()\n",
    "        spam.append(np.where(y_predicted_spo > 0.5, 1, 0))\n",
    "    else:\n",
    "        ar = np.array([Xnew[i]])\n",
    "        df10 = pd.DataFrame(ar, columns=['text'])\n",
    "        new_text_vector = vectorizer.transform(df10['text']).toarray()\n",
    "        y_predicted_polit = loaded_model.predict(new_text_vector)\n",
    "        y_predicted_polit = y_predicted_polit.flatten()\n",
    "        spam.append(np.where(y_predicted_polit > 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851d5c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f0ce1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfnew['spam']=pd.DataFrame(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ca081",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "New_df_emails = dfnew.to_csv('/kaggle/working/BBC-TEXT-NEW.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f52b05",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.30843,
   "end_time": "2023-09-22T02:29:48.939455",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-22T02:29:19.631025",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
